
TSL LLM Benchmark
=================

The files in this root directory are used for the generation of any state machine. For each particular state machine, there is a folder with the model-specific files.

Files:

1. run.py - run the TSL LLM generation process, with these options:
  a. directory - "Ball", "Shades", etc.
  b. method:
    from NL, and functions+pred term headers	
    from NL+TSL spec
    from NL+TSL spec+synthesized code	
1. Spec.prompt - gen the TSL spec from NL and functions+pred term headers
2. Impl.spec - gen the implementation, from NL summary, function+pred term headers, and the wrapper API description

Within each directory:

1. NL.txt - natural language description
2. Headers.txt - function+pred term headers

Works in Progress
==================

For models still in progress, there is a simpler organization while we experiment:

1. `[Name].Prompt.txt` represents the human-readable prompt to feed into the LLM to generate the spec
2. `[Name].Spec.tsl` is the TSL spec
3. `[Name].Synth.js` is the program generated by the TSL spec - this is NOT written by a human, only by the TSL generator or LLM
4. `[Name].Impl.js` - the implementation of functions referenced by the TSL spec.

For (4), we are trying to get the LLM to generate the implementation, and compare it to if the LLM generated everything from the start without TSL.
